# HW#9

Operation performed on the following hardware:

`````bash
                  -`
                 .o+`
                `ooo/
               `+oooo:
              `+oooooo:
              -+oooooo+:
            `/:-:++oooo+:
           `/++++/+++++++:
          `/++++++++++++++:
         `/+++ooooooooooooo/`
        ./ooosssso++osssssso+`
       .oossssso-````/ossssss+`
      -osssssso.      :ssssssso.
     :osssssss/        osssso+++.
    /ossssssss/        +ssssooo/-
  `/ossssso+/:-        -:/+osssso+-
 `+sso+:-`                 `.-/+oso:
`++:.                           `-/+/
.`                                 `/[1G[18A[41C[36m   Û∞Ñõ  „Ç≥„É≥„Éî„É•„Éº„Çø„Éº
[41C‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
[41C  Û∞á∫ Chassis : 1.0
[41C  Û∞£á OS : Arch Linux
[41C  Óâ≤ Kernel : 6.18.6-arch1-1
[41C  Û∞èó Packages : 1643 (pacman)
[41C  Û∞çπ Display : 1920x1080 @ 60Hz [External]
[41C  Û∞çπ Display : 2560x1440 @ 60Hz [External]
[41C  >_ Terminal : electron
[41C  Û±óÉ WM : Hyprland
[41C‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
[41C
[41C  Ó∑Ö : domai @ dpc
[41C‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
[41C  Ôíº CPU : AMD Ryzen 7 3700X @ 4.43 GHz
[41C  Û∞ä¥ GPU : NVIDIA GeForce RTX 2060 SUPER
[41C  ÔÄ© GPU Driver : nvidia (open source) 590.48.01
[41C  ÓøÖ  Memory : 12.45 GiB / 31.27 GiB (40%)
[41C  Û±¶ü OS Age  : 640 days
[41C  Û±´ê Uptime  : 4 hours, 31 mins
[41C‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
[41C  ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
[41C
`````

## Program Output

```bash
‚ùØ ./parallel_prefix_sum
Block scan: 512 threads -> 1024 elements per block
Timing repetitions: CPU=20, GPU=200

|   n   | CPU time [ms] | GPU time [ms] | Speedup |
|-------|---------------|---------------|---------|
| 100000 |         0.055 |         0.019 |    2.91x |
| 200000 |         0.109 |         0.030 |    3.59x |
| 300000 |         0.158 |         0.042 |    3.80x |
| 400000 |         0.195 |         0.055 |    3.53x |
| 500000 |         0.278 |         0.068 |    4.11x |
| 600000 |         0.332 |         0.080 |    4.17x |
| 700000 |         0.385 |         0.096 |    4.02x |
| 800000 |         0.448 |         0.105 |    4.25x |
| 900000 |         0.508 |         0.119 |    4.28x |
| 1000000 |         0.584 |         0.136 |    4.29x |
```

## Analysis & Explaination

### 1) CPU reference + GPU correctness check

No mismatch warnings are printed, so for all tested sizes `n = 100k ‚Ä¶ 1M` the GPU inclusive scan output matches the CPU scan (as required for validation). The CPU reference is the correct baseline for correctness and speedup reporting.

### 2) Brent‚ÄìKung work-efficient scan behavior

Your GPU implementation is the standard **work-efficient scan** structure: **up-sweep (reduce) + down-sweep**, done in shared memory per block segment, and then extended to large `n` by scanning per-block totals and applying offsets. This matches the canonical description of work-efficient scan used in CUDA literature. ([NVIDIA Developer][1])

### 3) Speedup table requirement (100k to 1M step 100k)

The printed table satisfies the requirement: 10 rows covering `100000 ‚Ä¶ 1000000` with a `100000` step.

---

## Performance analysis of the measured times

### Scaling with `n`

Both CPU and GPU times grow approximately linearly with `n`, which is expected for scan (overall work Œò(n)). The GPU times grow more slowly, indicating better throughput.

### Speedup trend

Speedup increases from **2.91√ó (100k)** to **4.29√ó (1M)** and then stabilizes around **~4.0‚Äì4.3√ó** for larger `n`.

Interpretation:

- For smaller `n`, fixed costs (kernel launch, extra kernels for block sums/offsets, synchronization) are a larger fraction of total time.
- As `n` grows, those fixed costs are amortized, so the speedup improves and approaches a plateau, which is typical for GPU scan implementations. ([NVIDIA Developer][1])

The small dips (e.g., at 400k, 700k) are consistent with normal measurement noise (clock variability, OS scheduling, cache state, etc.) at sub-millisecond timings.

### Memory-bandwidth-limited plateau

An integer scan has low arithmetic intensity: it must at least **read input + write output** (and does additional shared-memory traffic internally). Once `n` is large enough, performance tends to be dominated by memory throughput rather than arithmetic, so speedup saturates. This ‚Äúscan is bandwidth-bound in practice‚Äù behavior is a standard conclusion in CUDA scan references. ([NVIDIA Developer][1])

A rough throughput estimate using only global read+write of the main array (8 bytes/element):

- At **n = 1,000,000**, GPU time **0.136 ms** ‚Üí ~**58.8 GB/s** effective read+write throughput.
- At **n = 1,000,000**, CPU time **0.584 ms** ‚Üí ~**13.7 GB/s** effective read+write throughput.

That ratio aligns with the observed **~4.29√ó** speedup.
